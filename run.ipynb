{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Colab工作区挂载Google Drive\n","\n","将Google Drive挂载到Colab工作区于`/content/drive`目录下，并且将其路径添加到环境变量中，即切换当前环境在`LLM4HRSI`下"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31888,"status":"ok","timestamp":1716983944773,"user":{"displayName":"Kian Chen","userId":"02524605744360127868"},"user_tz":-480},"id":"s12AeQd5LCfg","outputId":"0a1454c8-198c-400d-883a-c7efa0ba2dcd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e0ejciFAP51_"},"outputs":[],"source":["import os\n","os.chdir('/content/gdrive/MyDrive/LLM4HRSI')"]},{"cell_type":"markdown","metadata":{},"source":["## 安装对应依赖\n","直接在Colab默认的环境下`pip install -r requirements.txt`也可以，但是好像`torch`有点小问题，可以把`requirements.txt`中`torch`那一条去掉。\n","\n","并且不知道是谁的问题，`einops`不能被一起下下来，不过可以单独安装。"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16630,"status":"ok","timestamp":1716984041860,"user":{"displayName":"Kian Chen","userId":"02524605744360127868"},"user_tz":-480},"id":"ttoCQUUILcZe","outputId":"58f2123a-dbba-46e5-bec8-caf80f0e2326"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting transformers==4.30.1 (from -r ./gdrive/MyDrive/LLM4HRSI/requirements.txt (line 2))\n","  Downloading transformers-4.30.1-py3-none-any.whl (7.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.1->-r ./gdrive/MyDrive/LLM4HRSI/requirements.txt (line 2)) (3.14.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.1->-r ./gdrive/MyDrive/LLM4HRSI/requirements.txt (line 2)) (0.23.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.1->-r ./gdrive/MyDrive/LLM4HRSI/requirements.txt (line 2)) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.1->-r ./gdrive/MyDrive/LLM4HRSI/requirements.txt (line 2)) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.1->-r ./gdrive/MyDrive/LLM4HRSI/requirements.txt (line 2)) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.1->-r ./gdrive/MyDrive/LLM4HRSI/requirements.txt (line 2)) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.1->-r ./gdrive/MyDrive/LLM4HRSI/requirements.txt (line 2)) (2.31.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.30.1->-r ./gdrive/MyDrive/LLM4HRSI/requirements.txt (line 2))\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.1->-r ./gdrive/MyDrive/LLM4HRSI/requirements.txt (line 2)) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.1->-r ./gdrive/MyDrive/LLM4HRSI/requirements.txt (line 2)) (4.66.4)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.1->-r ./gdrive/MyDrive/LLM4HRSI/requirements.txt (line 2)) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.1->-r ./gdrive/MyDrive/LLM4HRSI/requirements.txt (line 2)) (4.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.1->-r ./gdrive/MyDrive/LLM4HRSI/requirements.txt (line 2)) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.1->-r ./gdrive/MyDrive/LLM4HRSI/requirements.txt (line 2)) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.1->-r ./gdrive/MyDrive/LLM4HRSI/requirements.txt (line 2)) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.1->-r ./gdrive/MyDrive/LLM4HRSI/requirements.txt (line 2)) (2024.2.2)\n","Installing collected packages: tokenizers, transformers\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.19.1\n","    Uninstalling tokenizers-0.19.1:\n","      Successfully uninstalled tokenizers-0.19.1\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.41.1\n","    Uninstalling transformers-4.41.1:\n","      Successfully uninstalled transformers-4.41.1\n","Successfully installed tokenizers-0.13.3 transformers-4.30.1\n"]}],"source":["!sudo pip install -r ./requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6953,"status":"ok","timestamp":1716984070310,"user":{"displayName":"Kian Chen","userId":"02524605744360127868"},"user_tz":-480},"id":"rVdNhY_EOX4v","outputId":"03a5e768-f932-4804-e23b-d08fc0a1cbab"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting einops\n","  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: einops\n","Successfully installed einops-0.8.0\n"]}],"source":["!pip install einops"]},{"cell_type":"markdown","metadata":{},"source":["## 检验`torch`能不能用GPU加速"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1716984111166,"user":{"displayName":"Kian Chen","userId":"02524605744360127868"},"user_tz":-480},"id":"5XHB0Lb8MWQo","outputId":"1b8507c4-facd-4796-d733-832fa7047334"},"outputs":[{"data":{"text/plain":["True"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","torch.cuda.is_available()"]},{"cell_type":"markdown","metadata":{},"source":["## 运行项目"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":400819,"status":"ok","timestamp":1716991366474,"user":{"displayName":"Kian Chen","userId":"02524605744360127868"},"user_tz":-480},"id":"1sMhFS4pSXbN","outputId":"e40fc942-1200-4a61-a189-b1ee102da2a7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Args in experiment:\n","Namespace(task_name='imputation', is_training=1, model_id='cha_all1_down1_mask_0.1', model='LLM4HRSI', data='custom', root_path='./dataset/', data_path='cha_all1_down1.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=24, label_len=0, pred_len=0, seasonal_patterns='Monthly', mask_rate=0.1, anomaly_ratio=0.25, top_k=5, num_kernels=6, enc_in=576, dec_in=576, c_out=576, d_model=768, n_heads=8, e_layers=2, d_layers=1, d_ff=768, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, num_workers=10, itr=5, train_epochs=100, batch_size=16, patience=3, learning_rate=0.001, des='Exp', loss='MSE', lradj='type1', use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2', p_hidden_dims=[128, 128], p_hidden_layers=2, patch_size=1, stride=1, gpt_layers=1, ln=0, mlp=0, weight=0, percent=1500)\n","Use GPU: cuda:0\n",">>>>>>>start training : imputation_cha_all1_down1_mask_0.1_LLM4HRSI_custom_ftM_te100_bs16_gl1_dm768_nh8_el2_lr0.001_enci576_df768_fc1_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n","0.1\n","train 7949\n","0.1\n","val 799\n","0.1\n","test 798\n","99it [00:14,  7.50it/s]\titers: 100, epoch: 1 | loss: 0.0784047\n","\tspeed: 0.1547s/iter; left time: 7660.1954s\n","199it [00:28,  7.34it/s]\titers: 200, epoch: 1 | loss: 0.0840261\n","\tspeed: 0.1352s/iter; left time: 6676.9928s\n","299it [00:41,  7.23it/s]\titers: 300, epoch: 1 | loss: 0.0892665\n","\tspeed: 0.1369s/iter; left time: 6747.4308s\n","399it [00:55,  7.21it/s]\titers: 400, epoch: 1 | loss: 0.0589243\n","\tspeed: 0.1393s/iter; left time: 6854.2810s\n","496it [01:09,  7.12it/s]\n","Epoch: 1 cost time: 69.90434741973877\n","49it [00:03, 14.99it/s]\n","49it [00:03, 14.47it/s]\n","Epoch: 1, Steps: 496 | Train Loss: 0.1074174 Vali Loss: 0.0567413 Test Loss: 0.0538542\n","Validation loss decreased (inf --> 0.056741).  Saving model ...\n","Updating learning rate to 0.001\n","99it [00:14,  6.93it/s]\titers: 100, epoch: 2 | loss: 0.0626981\n","\tspeed: 0.3686s/iter; left time: 18061.2113s\n","199it [00:28,  6.65it/s]\titers: 200, epoch: 2 | loss: 0.0610002\n","\tspeed: 0.1469s/iter; left time: 7184.6581s\n","299it [00:44,  6.33it/s]\titers: 300, epoch: 2 | loss: 0.0850218\n","\tspeed: 0.1534s/iter; left time: 7484.8471s\n","399it [01:00,  6.36it/s]\titers: 400, epoch: 2 | loss: 0.0530386\n","\tspeed: 0.1583s/iter; left time: 7710.1055s\n","496it [01:15,  6.60it/s]\n","Epoch: 2 cost time: 75.67800378799438\n","49it [00:03, 14.06it/s]\n","49it [00:03, 15.04it/s]\n","Epoch: 2, Steps: 496 | Train Loss: 0.0751005 Vali Loss: 0.0529712 Test Loss: 0.0501414\n","Validation loss decreased (0.056741 --> 0.052971).  Saving model ...\n","Updating learning rate to 0.0005\n","99it [00:14,  6.75it/s]\titers: 100, epoch: 3 | loss: 0.0426141\n","\tspeed: 0.3824s/iter; left time: 18550.8628s\n","199it [00:29,  6.66it/s]\titers: 200, epoch: 3 | loss: 0.0439664\n","\tspeed: 0.1488s/iter; left time: 7201.4521s\n","299it [00:44,  6.58it/s]\titers: 300, epoch: 3 | loss: 0.0685584\n","\tspeed: 0.1516s/iter; left time: 7323.9675s\n","399it [00:59,  6.58it/s]\titers: 400, epoch: 3 | loss: 0.0539657\n","\tspeed: 0.1532s/iter; left time: 7384.9299s\n","496it [01:14,  6.62it/s]\n","Epoch: 3 cost time: 75.22326326370239\n","49it [00:03, 14.85it/s]\n","49it [00:03, 14.49it/s]\n","Epoch: 3, Steps: 496 | Train Loss: 0.0683660 Vali Loss: 0.0489144 Test Loss: 0.0463986\n","Validation loss decreased (0.052971 --> 0.048914).  Saving model ...\n","Updating learning rate to 0.00025\n","99it [00:14,  6.73it/s]\titers: 100, epoch: 4 | loss: 0.0375071\n","\tspeed: 0.3834s/iter; left time: 18409.8613s\n","199it [00:29,  6.67it/s]\titers: 200, epoch: 4 | loss: 0.0321135\n","\tspeed: 0.1501s/iter; left time: 7189.5495s\n","299it [00:44,  6.59it/s]\titers: 300, epoch: 4 | loss: 0.0542650\n","\tspeed: 0.1512s/iter; left time: 7231.1448s\n","399it [01:00,  6.58it/s]\titers: 400, epoch: 4 | loss: 0.0478047\n","\tspeed: 0.1518s/iter; left time: 7241.5581s\n","496it [01:14,  6.61it/s]\n","Epoch: 4 cost time: 75.45967841148376\n","49it [00:03, 13.82it/s]\n","49it [00:03, 14.94it/s]\n","Epoch: 4, Steps: 496 | Train Loss: 0.0548747 Vali Loss: 0.0442532 Test Loss: 0.0426020\n","Validation loss decreased (0.048914 --> 0.044253).  Saving model ...\n","Updating learning rate to 0.000125\n","99it [00:14,  6.63it/s]\titers: 100, epoch: 5 | loss: 0.0233897\n","\tspeed: 0.3831s/iter; left time: 18205.6238s\n","199it [00:29,  6.57it/s]\titers: 200, epoch: 5 | loss: 0.0285721\n","\tspeed: 0.1505s/iter; left time: 7137.3161s\n","299it [00:44,  6.68it/s]\titers: 300, epoch: 5 | loss: 0.0423988\n","\tspeed: 0.1507s/iter; left time: 7128.4059s\n","399it [01:00,  6.59it/s]\titers: 400, epoch: 5 | loss: 0.0400406\n","\tspeed: 0.1515s/iter; left time: 7152.2516s\n","496it [01:14,  6.62it/s]\n","Epoch: 5 cost time: 75.23460364341736\n","49it [00:03, 15.06it/s]\n","49it [00:03, 15.02it/s]\n","Epoch: 5, Steps: 496 | Train Loss: 0.0419866 Vali Loss: 0.0377553 Test Loss: 0.0369474\n","Validation loss decreased (0.044253 --> 0.037755).  Saving model ...\n","Updating learning rate to 6.25e-05\n","99it [00:14,  6.68it/s]\titers: 100, epoch: 6 | loss: 0.0210824\n","\tspeed: 0.3816s/iter; left time: 17943.5488s\n","199it [00:29,  6.66it/s]\titers: 200, epoch: 6 | loss: 0.0266622\n","\tspeed: 0.1503s/iter; left time: 7054.4478s\n","299it [00:44,  6.66it/s]\titers: 300, epoch: 6 | loss: 0.0395004\n","\tspeed: 0.1512s/iter; left time: 7078.8940s\n","399it [01:00,  6.58it/s]\titers: 400, epoch: 6 | loss: 0.0390516\n","\tspeed: 0.1514s/iter; left time: 7073.5641s\n","496it [01:15,  6.61it/s]\n","Epoch: 6 cost time: 75.49191284179688\n","49it [00:03, 13.81it/s]\n","49it [00:03, 14.91it/s]\n","Epoch: 6, Steps: 496 | Train Loss: 0.0383297 Vali Loss: 0.0352967 Test Loss: 0.0353991\n","Validation loss decreased (0.037755 --> 0.035297).  Saving model ...\n","Updating learning rate to 3.125e-05\n","99it [00:14,  6.65it/s]\titers: 100, epoch: 7 | loss: 0.0199651\n","\tspeed: 0.3891s/iter; left time: 18101.4565s\n","199it [00:29,  6.60it/s]\titers: 200, epoch: 7 | loss: 0.0259166\n","\tspeed: 0.1511s/iter; left time: 7013.7348s\n","299it [00:45,  6.53it/s]\titers: 300, epoch: 7 | loss: 0.0358672\n","\tspeed: 0.1519s/iter; left time: 7036.0228s\n","399it [01:00,  6.65it/s]\titers: 400, epoch: 7 | loss: 0.0379248\n","\tspeed: 0.1512s/iter; left time: 6989.5908s\n","496it [01:15,  6.61it/s]\n","Epoch: 7 cost time: 75.29255247116089\n","49it [00:03, 14.78it/s]\n","49it [00:03, 14.25it/s]\n","Epoch: 7, Steps: 496 | Train Loss: 0.0367859 Vali Loss: 0.0342937 Test Loss: 0.0349567\n","Validation loss decreased (0.035297 --> 0.034294).  Saving model ...\n","Updating learning rate to 1.5625e-05\n","99it [00:14,  6.66it/s]\titers: 100, epoch: 8 | loss: 0.0183903\n","\tspeed: 0.3843s/iter; left time: 17688.5986s\n","199it [00:29,  6.56it/s]\titers: 200, epoch: 8 | loss: 0.0250719\n","\tspeed: 0.1506s/iter; left time: 6916.1839s\n","299it [00:45,  6.58it/s]\titers: 300, epoch: 8 | loss: 0.0341376\n","\tspeed: 0.1518s/iter; left time: 6957.1619s\n","399it [01:00,  6.69it/s]\titers: 400, epoch: 8 | loss: 0.0378222\n","\tspeed: 0.1513s/iter; left time: 6920.5843s\n","496it [01:14,  6.62it/s]\n","Epoch: 8 cost time: 75.38706350326538\n","49it [00:03, 15.00it/s]\n","49it [00:03, 14.95it/s]\n","Epoch: 8, Steps: 496 | Train Loss: 0.0359012 Vali Loss: 0.0338410 Test Loss: 0.0348901\n","Validation loss decreased (0.034294 --> 0.033841).  Saving model ...\n","Updating learning rate to 7.8125e-06\n","99it [00:14,  6.70it/s]\titers: 100, epoch: 9 | loss: 0.0196313\n","\tspeed: 0.3751s/iter; left time: 17078.0039s\n","199it [00:29,  6.64it/s]\titers: 200, epoch: 9 | loss: 0.0250748\n","\tspeed: 0.1502s/iter; left time: 6823.5738s\n","299it [00:44,  6.65it/s]\titers: 300, epoch: 9 | loss: 0.0318803\n","\tspeed: 0.1503s/iter; left time: 6812.7860s\n","399it [00:59,  6.63it/s]\titers: 400, epoch: 9 | loss: 0.0374820\n","\tspeed: 0.1503s/iter; left time: 6799.4561s\n","496it [01:14,  6.65it/s]\n","Epoch: 9 cost time: 74.8306131362915\n","49it [00:03, 14.41it/s]\n","49it [00:03, 13.98it/s]\n","Epoch: 9, Steps: 496 | Train Loss: 0.0354121 Vali Loss: 0.0337171 Test Loss: 0.0349149\n","Validation loss decreased (0.033841 --> 0.033717).  Saving model ...\n","Updating learning rate to 3.90625e-06\n","99it [00:14,  6.65it/s]\titers: 100, epoch: 10 | loss: 0.0198204\n","\tspeed: 0.3812s/iter; left time: 17167.1479s\n","199it [00:29,  6.65it/s]\titers: 200, epoch: 10 | loss: 0.0247851\n","\tspeed: 0.1512s/iter; left time: 6795.2399s\n","299it [00:45,  6.56it/s]\titers: 300, epoch: 10 | loss: 0.0323929\n","\tspeed: 0.1518s/iter; left time: 6806.3216s\n","399it [01:00,  6.59it/s]\titers: 400, epoch: 10 | loss: 0.0377913\n","\tspeed: 0.1520s/iter; left time: 6799.5470s\n","496it [01:15,  6.60it/s]\n","Epoch: 10 cost time: 75.44269967079163\n","49it [00:03, 14.96it/s]\n","49it [00:03, 14.98it/s]\n","Epoch: 10, Steps: 496 | Train Loss: 0.0351984 Vali Loss: 0.0336912 Test Loss: 0.0349992\n","Validation loss decreased (0.033717 --> 0.033691).  Saving model ...\n","Updating learning rate to 1.953125e-06\n","99it [00:14,  6.66it/s]\titers: 100, epoch: 11 | loss: 0.0182053\n","\tspeed: 0.3760s/iter; left time: 16745.7891s\n","199it [00:29,  6.58it/s]\titers: 200, epoch: 11 | loss: 0.0246869\n","\tspeed: 0.1507s/iter; left time: 6697.3748s\n","299it [00:45,  6.54it/s]\titers: 300, epoch: 11 | loss: 0.0332472\n","\tspeed: 0.1519s/iter; left time: 6737.4544s\n","399it [01:00,  6.58it/s]\titers: 400, epoch: 11 | loss: 0.0371256\n","\tspeed: 0.1519s/iter; left time: 6719.5330s\n","496it [01:15,  6.61it/s]\n","Epoch: 11 cost time: 75.35462379455566\n","49it [00:03, 15.07it/s]\n","49it [00:03, 14.65it/s]\n","Epoch: 11, Steps: 496 | Train Loss: 0.0349767 Vali Loss: 0.0337775 Test Loss: 0.0350635\n","EarlyStopping counter: 1 out of 3\n","Updating learning rate to 9.765625e-07\n","99it [00:14,  6.68it/s]\titers: 100, epoch: 12 | loss: 0.0186269\n","\tspeed: 0.3729s/iter; left time: 16423.0835s\n","199it [00:29,  6.60it/s]\titers: 200, epoch: 12 | loss: 0.0245600\n","\tspeed: 0.1505s/iter; left time: 6614.2000s\n","299it [00:45,  6.57it/s]\titers: 300, epoch: 12 | loss: 0.0312019\n","\tspeed: 0.1518s/iter; left time: 6656.5257s\n","399it [01:00,  6.58it/s]\titers: 400, epoch: 12 | loss: 0.0370582\n","\tspeed: 0.1517s/iter; left time: 6636.3275s\n","496it [01:15,  6.61it/s]\n","Epoch: 12 cost time: 75.46215415000916\n","49it [00:03, 14.03it/s]\n","49it [00:03, 15.08it/s]\n","Epoch: 12, Steps: 496 | Train Loss: 0.0348866 Vali Loss: 0.0339191 Test Loss: 0.0351325\n","EarlyStopping counter: 2 out of 3\n","Updating learning rate to 4.8828125e-07\n","99it [00:14,  6.72it/s]\titers: 100, epoch: 13 | loss: 0.0169989\n","\tspeed: 0.3742s/iter; left time: 16294.1223s\n","199it [00:29,  6.66it/s]\titers: 200, epoch: 13 | loss: 0.0244737\n","\tspeed: 0.1495s/iter; left time: 6494.4317s\n","299it [00:44,  6.61it/s]\titers: 300, epoch: 13 | loss: 0.0323910\n","\tspeed: 0.1503s/iter; left time: 6516.9194s\n","399it [00:59,  6.61it/s]\titers: 400, epoch: 13 | loss: 0.0375072\n","\tspeed: 0.1514s/iter; left time: 6548.5772s\n","496it [01:14,  6.64it/s]\n","Epoch: 13 cost time: 75.01251268386841\n","49it [00:03, 14.82it/s]\n","49it [00:03, 14.51it/s]\n","Epoch: 13, Steps: 496 | Train Loss: 0.0348786 Vali Loss: 0.0340161 Test Loss: 0.0351817\n","EarlyStopping counter: 3 out of 3\n","Early stopping\n",">>>>>>>testing : imputation_cha_all1_down1_mask_0.1_LLM4HRSI_custom_ftM_te100_bs16_gl1_dm768_nh8_el2_lr0.001_enci576_df768_fc1_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n","0.1\n","test 798\n","49it [00:04, 11.17it/s]\n","test shape: (784, 24, 576) (784, 24, 576)\n","mse:0.0036469795741140842, mae:0.036618947982788086,rmse:0.060390226542949677\n","Use GPU: cuda:0\n",">>>>>>>start training : imputation_cha_all1_down1_mask_0.1_LLM4HRSI_custom_ftM_te100_bs16_gl1_dm768_nh8_el2_lr0.001_enci576_df768_fc1_ebtimeF_dtTrue_Exp_1>>>>>>>>>>>>>>>>>>>>>>>>>>\n","0.1\n","train 7949\n","0.1\n","val 799\n","0.1\n","test 798\n","99it [00:13,  7.11it/s]\titers: 100, epoch: 1 | loss: 0.0706451\n","\tspeed: 0.1462s/iter; left time: 7235.0924s\n","199it [00:28,  7.04it/s]\titers: 200, epoch: 1 | loss: 0.0886917\n","\tspeed: 0.1413s/iter; left time: 6978.3659s\n","299it [00:42,  7.04it/s]\titers: 300, epoch: 1 | loss: 0.1050972\n","\tspeed: 0.1418s/iter; left time: 6993.0956s\n","399it [00:56,  7.09it/s]\titers: 400, epoch: 1 | loss: 0.0604113\n","\tspeed: 0.1420s/iter; left time: 6984.1965s\n","496it [01:10,  7.04it/s]\n","Epoch: 1 cost time: 71.03320622444153\n","49it [00:03, 15.59it/s]\n","49it [00:03, 15.50it/s]\n","Epoch: 1, Steps: 496 | Train Loss: 0.1033794 Vali Loss: 0.0563795 Test Loss: 0.0539167\n","Validation loss decreased (inf --> 0.056379).  Saving model ...\n","Updating learning rate to 0.001\n","99it [00:14,  6.90it/s]\titers: 100, epoch: 2 | loss: 0.0469016\n","\tspeed: 0.3637s/iter; left time: 17821.0852s\n","199it [00:28,  6.96it/s]\titers: 200, epoch: 2 | loss: 0.0593378\n","\tspeed: 0.1442s/iter; left time: 7052.8486s\n","299it [00:43,  6.80it/s]\titers: 300, epoch: 2 | loss: 0.0746292\n","\tspeed: 0.1445s/iter; left time: 7053.9183s\n","399it [00:57,  6.86it/s]\titers: 400, epoch: 2 | loss: 0.0550918\n","\tspeed: 0.1454s/iter; left time: 7080.7830s\n","496it [01:11,  6.89it/s]\n","Epoch: 2 cost time: 72.34048128128052\n","49it [00:03, 15.38it/s]\n","49it [00:03, 14.72it/s]\n","Epoch: 2, Steps: 496 | Train Loss: 0.0757425 Vali Loss: 0.0537017 Test Loss: 0.0512215\n","Validation loss decreased (0.056379 --> 0.053702).  Saving model ...\n","Updating learning rate to 0.0005\n","99it [00:14,  6.78it/s]\titers: 100, epoch: 3 | loss: 0.0444130\n","\tspeed: 0.3745s/iter; left time: 18166.9974s\n","199it [00:29,  6.88it/s]\titers: 200, epoch: 3 | loss: 0.0443216\n","\tspeed: 0.1467s/iter; left time: 7103.0941s\n","299it [00:43,  6.77it/s]\titers: 300, epoch: 3 | loss: 0.0656570\n","\tspeed: 0.1473s/iter; left time: 7118.1415s\n","399it [00:58,  6.78it/s]\titers: 400, epoch: 3 | loss: 0.0594655\n","\tspeed: 0.1478s/iter; left time: 7126.8923s\n","496it [01:13,  6.76it/s]\n","Epoch: 3 cost time: 73.81983137130737\n","49it [00:03, 14.77it/s]\n","49it [00:03, 15.13it/s]\n","Epoch: 3, Steps: 496 | Train Loss: 0.0675148 Vali Loss: 0.0532792 Test Loss: 0.0500912\n","Validation loss decreased (0.053702 --> 0.053279).  Saving model ...\n","Updating learning rate to 0.00025\n","99it [00:14,  6.74it/s]\titers: 100, epoch: 4 | loss: 0.0293632\n","\tspeed: 0.3771s/iter; left time: 18106.0418s\n","199it [00:29,  6.67it/s]\titers: 200, epoch: 4 | loss: 0.0326165\n","\tspeed: 0.1485s/iter; left time: 7117.0296s\n","299it [00:44,  6.72it/s]\titers: 300, epoch: 4 | loss: 0.0565255\n","\tspeed: 0.1489s/iter; left time: 7121.2272s\n","399it [00:59,  6.72it/s]\titers: 400, epoch: 4 | loss: 0.0451489\n","\tspeed: 0.1488s/iter; left time: 7097.6926s\n","496it [01:13,  6.72it/s]\n","Epoch: 4 cost time: 74.19293665885925\n","49it [00:03, 15.11it/s]\n","49it [00:03, 15.05it/s]\n","Epoch: 4, Steps: 496 | Train Loss: 0.0532100 Vali Loss: 0.0449667 Test Loss: 0.0431938\n","Validation loss decreased (0.053279 --> 0.044967).  Saving model ...\n","Updating learning rate to 0.000125\n","99it [00:14,  6.76it/s]\titers: 100, epoch: 5 | loss: 0.0248142\n","\tspeed: 0.3763s/iter; left time: 17880.6005s\n","199it [00:29,  6.73it/s]\titers: 200, epoch: 5 | loss: 0.0294806\n","\tspeed: 0.1485s/iter; left time: 7043.6748s\n","299it [00:44,  6.77it/s]\titers: 300, epoch: 5 | loss: 0.0476307\n","\tspeed: 0.1487s/iter; left time: 7037.3121s\n","399it [00:59,  6.68it/s]\titers: 400, epoch: 5 | loss: 0.0422188\n","\tspeed: 0.1488s/iter; left time: 7024.0506s\n","496it [01:13,  6.70it/s]\n","Epoch: 5 cost time: 74.54090023040771\n","49it [00:03, 13.85it/s]\n","49it [00:03, 15.07it/s]\n","Epoch: 5, Steps: 496 | Train Loss: 0.0426174 Vali Loss: 0.0394319 Test Loss: 0.0389458\n","Validation loss decreased (0.044967 --> 0.039432).  Saving model ...\n","Updating learning rate to 6.25e-05\n","99it [00:14,  6.72it/s]\titers: 100, epoch: 6 | loss: 0.0252251\n","\tspeed: 0.3818s/iter; left time: 17950.8770s\n","199it [00:29,  6.73it/s]\titers: 200, epoch: 6 | loss: 0.0282590\n","\tspeed: 0.1491s/iter; left time: 6998.0218s\n","299it [00:44,  6.66it/s]\titers: 300, epoch: 6 | loss: 0.0456041\n","\tspeed: 0.1497s/iter; left time: 7009.5039s\n","399it [00:59,  6.67it/s]\titers: 400, epoch: 6 | loss: 0.0405846\n","\tspeed: 0.1497s/iter; left time: 6992.4465s\n","496it [01:14,  6.68it/s]\n","Epoch: 6 cost time: 74.62024188041687\n","49it [00:03, 15.09it/s]\n","49it [00:03, 14.63it/s]\n","Epoch: 6, Steps: 496 | Train Loss: 0.0401573 Vali Loss: 0.0372115 Test Loss: 0.0375996\n","Validation loss decreased (0.039432 --> 0.037212).  Saving model ...\n","Updating learning rate to 3.125e-05\n","99it [00:14,  6.65it/s]\titers: 100, epoch: 7 | loss: 0.0237879\n","\tspeed: 0.3813s/iter; left time: 17740.2389s\n","199it [00:29,  6.72it/s]\titers: 200, epoch: 7 | loss: 0.0276853\n","\tspeed: 0.1488s/iter; left time: 6906.1047s\n","299it [00:44,  6.72it/s]\titers: 300, epoch: 7 | loss: 0.0408979\n","\tspeed: 0.1489s/iter; left time: 6898.7251s\n","399it [00:59,  6.71it/s]\titers: 400, epoch: 7 | loss: 0.0403787\n","\tspeed: 0.1490s/iter; left time: 6888.2579s\n","496it [01:14,  6.70it/s]\n","Epoch: 7 cost time: 74.55236506462097\n","49it [00:03, 14.70it/s]\n","49it [00:03, 13.93it/s]\n","Epoch: 7, Steps: 496 | Train Loss: 0.0389774 Vali Loss: 0.0364147 Test Loss: 0.0369773\n","Validation loss decreased (0.037212 --> 0.036415).  Saving model ...\n","Updating learning rate to 1.5625e-05\n","99it [00:14,  6.69it/s]\titers: 100, epoch: 8 | loss: 0.0209963\n","\tspeed: 0.3809s/iter; left time: 17534.1521s\n","199it [00:29,  6.64it/s]\titers: 200, epoch: 8 | loss: 0.0267433\n","\tspeed: 0.1489s/iter; left time: 6837.7882s\n","299it [00:44,  6.68it/s]\titers: 300, epoch: 8 | loss: 0.0391574\n","\tspeed: 0.1499s/iter; left time: 6867.7763s\n","399it [00:59,  6.63it/s]\titers: 400, epoch: 8 | loss: 0.0399215\n","\tspeed: 0.1503s/iter; left time: 6873.1631s\n","496it [01:14,  6.66it/s]\n","Epoch: 8 cost time: 74.83258366584778\n","49it [00:03, 14.95it/s]\n","49it [00:03, 14.99it/s]\n","Epoch: 8, Steps: 496 | Train Loss: 0.0377547 Vali Loss: 0.0363133 Test Loss: 0.0368615\n","Validation loss decreased (0.036415 --> 0.036313).  Saving model ...\n","Updating learning rate to 7.8125e-06\n","99it [00:14,  6.70it/s]\titers: 100, epoch: 9 | loss: 0.0187440\n","\tspeed: 0.3781s/iter; left time: 17216.3505s\n","199it [00:29,  6.72it/s]\titers: 200, epoch: 9 | loss: 0.0265570\n","\tspeed: 0.1491s/iter; left time: 6775.9791s\n","299it [00:44,  6.70it/s]\titers: 300, epoch: 9 | loss: 0.0378759\n","\tspeed: 0.1493s/iter; left time: 6769.6385s\n","399it [00:59,  6.69it/s]\titers: 400, epoch: 9 | loss: 0.0395451\n","\tspeed: 0.1493s/iter; left time: 6754.4152s\n","496it [01:14,  6.68it/s]\n","Epoch: 9 cost time: 74.55600118637085\n","49it [00:03, 14.44it/s]\n","49it [00:03, 13.89it/s]\n","Epoch: 9, Steps: 496 | Train Loss: 0.0372644 Vali Loss: 0.0361265 Test Loss: 0.0368869\n","Validation loss decreased (0.036313 --> 0.036127).  Saving model ...\n","Updating learning rate to 3.90625e-06\n","99it [00:14,  6.76it/s]\titers: 100, epoch: 10 | loss: 0.0191164\n","\tspeed: 0.3828s/iter; left time: 17241.8977s\n","199it [00:29,  6.72it/s]\titers: 200, epoch: 10 | loss: 0.0278993\n","\tspeed: 0.1492s/iter; left time: 6703.9222s\n","299it [00:44,  6.70it/s]\titers: 300, epoch: 10 | loss: 0.0369609\n","\tspeed: 0.1494s/iter; left time: 6700.4861s\n","399it [00:59,  6.68it/s]\titers: 400, epoch: 10 | loss: 0.0395471\n","\tspeed: 0.1498s/iter; left time: 6699.9725s\n","496it [01:14,  6.68it/s]\n","Epoch: 10 cost time: 74.66413450241089\n","49it [00:03, 14.99it/s]\n","49it [00:03, 14.94it/s]\n","Epoch: 10, Steps: 496 | Train Loss: 0.0370243 Vali Loss: 0.0360851 Test Loss: 0.0369807\n","Validation loss decreased (0.036127 --> 0.036085).  Saving model ...\n","Updating learning rate to 1.953125e-06\n","99it [00:14,  6.77it/s]\titers: 100, epoch: 11 | loss: 0.0188761\n","\tspeed: 0.3761s/iter; left time: 16753.4684s\n","199it [00:29,  6.70it/s]\titers: 200, epoch: 11 | loss: 0.0258375\n","\tspeed: 0.1491s/iter; left time: 6624.6525s\n","299it [00:44,  6.70it/s]\titers: 300, epoch: 11 | loss: 0.0359246\n","\tspeed: 0.1492s/iter; left time: 6617.4611s\n","399it [00:59,  6.68it/s]\titers: 400, epoch: 11 | loss: 0.0397345\n","\tspeed: 0.1497s/iter; left time: 6622.8455s\n","496it [01:14,  6.69it/s]\n","Epoch: 11 cost time: 74.53685307502747\n","49it [00:03, 14.72it/s]\n","49it [00:03, 13.99it/s]\n","Epoch: 11, Steps: 496 | Train Loss: 0.0369320 Vali Loss: 0.0361643 Test Loss: 0.0370385\n","EarlyStopping counter: 1 out of 3\n","Updating learning rate to 9.765625e-07\n","99it [00:14,  6.72it/s]\titers: 100, epoch: 12 | loss: 0.0210616\n","\tspeed: 0.3740s/iter; left time: 16474.0683s\n","199it [00:29,  6.62it/s]\titers: 200, epoch: 12 | loss: 0.0258188\n","\tspeed: 0.1490s/iter; left time: 6549.5198s\n","299it [00:44,  6.66it/s]\titers: 300, epoch: 12 | loss: 0.0354168\n","\tspeed: 0.1494s/iter; left time: 6548.8689s\n","399it [00:59,  6.70it/s]\titers: 400, epoch: 12 | loss: 0.0395210\n","\tspeed: 0.1497s/iter; left time: 6549.6755s\n","496it [01:14,  6.68it/s]\n","Epoch: 12 cost time: 74.70547413825989\n","49it [00:03, 15.13it/s]\n","49it [00:03, 15.05it/s]\n","Epoch: 12, Steps: 496 | Train Loss: 0.0367559 Vali Loss: 0.0363137 Test Loss: 0.0370965\n","EarlyStopping counter: 2 out of 3\n","Updating learning rate to 4.8828125e-07\n","99it [00:14,  6.69it/s]\titers: 100, epoch: 13 | loss: 0.0205547\n","\tspeed: 0.3702s/iter; left time: 16121.6507s\n","199it [00:29,  6.74it/s]\titers: 200, epoch: 13 | loss: 0.0260560\n","\tspeed: 0.1494s/iter; left time: 6490.0583s\n","299it [00:44,  6.67it/s]\titers: 300, epoch: 13 | loss: 0.0370452\n","\tspeed: 0.1498s/iter; left time: 6493.5638s\n","399it [00:59,  6.60it/s]\titers: 400, epoch: 13 | loss: 0.0395123\n","\tspeed: 0.1501s/iter; left time: 6490.0586s\n","496it [01:14,  6.67it/s]\n","Epoch: 13 cost time: 74.6688780784607\n","49it [00:03, 15.04it/s]\n","49it [00:03, 14.35it/s]\n","Epoch: 13, Steps: 496 | Train Loss: 0.0366560 Vali Loss: 0.0363988 Test Loss: 0.0371290\n","EarlyStopping counter: 3 out of 3\n","Early stopping\n",">>>>>>>testing : imputation_cha_all1_down1_mask_0.1_LLM4HRSI_custom_ftM_te100_bs16_gl1_dm768_nh8_el2_lr0.001_enci576_df768_fc1_ebtimeF_dtTrue_Exp_1<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n","0.1\n","test 798\n","49it [00:04, 11.28it/s]\n","test shape: (784, 24, 576) (784, 24, 576)\n","mse:0.003936101216822863, mae:0.03837523236870766,rmse:0.06273835152387619\n","Use GPU: cuda:0\n",">>>>>>>start training : imputation_cha_all1_down1_mask_0.1_LLM4HRSI_custom_ftM_te100_bs16_gl1_dm768_nh8_el2_lr0.001_enci576_df768_fc1_ebtimeF_dtTrue_Exp_2>>>>>>>>>>>>>>>>>>>>>>>>>>\n","0.1\n","train 7949\n","0.1\n","val 799\n","0.1\n","test 798\n","99it [00:13,  7.09it/s]\titers: 100, epoch: 1 | loss: 0.0855581\n","\tspeed: 0.1423s/iter; left time: 7044.3703s\n","199it [00:27,  6.97it/s]\titers: 200, epoch: 1 | loss: 0.0912516\n","\tspeed: 0.1409s/iter; left time: 6959.2605s\n","299it [00:42,  7.02it/s]\titers: 300, epoch: 1 | loss: 0.0992062\n","\tspeed: 0.1420s/iter; left time: 6998.6656s\n","399it [00:56,  7.09it/s]\titers: 400, epoch: 1 | loss: 0.0627522\n","\tspeed: 0.1418s/iter; left time: 6979.0084s\n","496it [01:10,  7.06it/s]\n","Epoch: 1 cost time: 70.59534215927124\n","49it [00:03, 15.27it/s]\n","49it [00:03, 15.24it/s]\n","Epoch: 1, Steps: 496 | Train Loss: 0.1070294 Vali Loss: 0.0564991 Test Loss: 0.0532555\n","Validation loss decreased (inf --> 0.056499).  Saving model ...\n","Updating learning rate to 0.001\n","99it [00:14,  6.97it/s]\titers: 100, epoch: 2 | loss: 0.0542520\n","\tspeed: 0.3657s/iter; left time: 17922.4339s\n","199it [00:28,  6.98it/s]\titers: 200, epoch: 2 | loss: 0.0645650\n","\tspeed: 0.1449s/iter; left time: 7088.1822s\n","299it [00:43,  6.92it/s]\titers: 300, epoch: 2 | loss: 0.0752822\n","\tspeed: 0.1444s/iter; left time: 7045.6753s\n","399it [00:57,  6.86it/s]\titers: 400, epoch: 2 | loss: 0.0546164\n","\tspeed: 0.1455s/iter; left time: 7086.8685s\n","496it [01:12,  6.87it/s]\n","Epoch: 2 cost time: 72.53577494621277\n","49it [00:03, 15.14it/s]\n","49it [00:03, 14.06it/s]\n","Epoch: 2, Steps: 496 | Train Loss: 0.0774314 Vali Loss: 0.0519337 Test Loss: 0.0509458\n","Validation loss decreased (0.056499 --> 0.051934).  Saving model ...\n","Updating learning rate to 0.0005\n","99it [00:14,  6.79it/s]\titers: 100, epoch: 3 | loss: 0.0390735\n","\tspeed: 0.3738s/iter; left time: 18132.2654s\n","199it [00:29,  6.75it/s]\titers: 200, epoch: 3 | loss: 0.0411466\n","\tspeed: 0.1467s/iter; left time: 7101.8163s\n","299it [00:43,  6.70it/s]\titers: 300, epoch: 3 | loss: 0.0679957\n","\tspeed: 0.1475s/iter; left time: 7123.3671s\n","399it [00:58,  6.79it/s]\titers: 400, epoch: 3 | loss: 0.0592559\n","\tspeed: 0.1481s/iter; left time: 7141.4702s\n","496it [01:13,  6.76it/s]\n","Epoch: 3 cost time: 73.69977569580078\n","49it [00:03, 13.92it/s]\n","49it [00:03, 15.12it/s]\n","Epoch: 3, Steps: 496 | Train Loss: 0.0753087 Vali Loss: 0.0492880 Test Loss: 0.0468377\n","Validation loss decreased (0.051934 --> 0.049288).  Saving model ...\n","Updating learning rate to 0.00025\n","99it [00:14,  6.74it/s]\titers: 100, epoch: 4 | loss: 0.0411560\n","\tspeed: 0.3827s/iter; left time: 18375.7468s\n","199it [00:29,  6.71it/s]\titers: 200, epoch: 4 | loss: 0.0330991\n","\tspeed: 0.1483s/iter; left time: 7103.5367s\n","299it [00:44,  6.67it/s]\titers: 300, epoch: 4 | loss: 0.0520119\n","\tspeed: 0.1491s/iter; left time: 7127.0020s\n","399it [00:59,  6.68it/s]\titers: 400, epoch: 4 | loss: 0.0465435\n","\tspeed: 0.1501s/iter; left time: 7163.6886s\n","496it [01:14,  6.68it/s]\n","Epoch: 4 cost time: 74.56824803352356\n","49it [00:03, 14.21it/s]\n","49it [00:03, 13.99it/s]\n","Epoch: 4, Steps: 496 | Train Loss: 0.0562279 Vali Loss: 0.0467489 Test Loss: 0.0451703\n","Validation loss decreased (0.049288 --> 0.046749).  Saving model ...\n","Updating learning rate to 0.000125\n","99it [00:14,  6.66it/s]\titers: 100, epoch: 5 | loss: 0.0261908\n","\tspeed: 0.3837s/iter; left time: 18231.1465s\n","199it [00:29,  6.66it/s]\titers: 200, epoch: 5 | loss: 0.0305540\n","\tspeed: 0.1510s/iter; left time: 7158.4902s\n","299it [00:45,  6.61it/s]\titers: 300, epoch: 5 | loss: 0.0511874\n","\tspeed: 0.1517s/iter; left time: 7177.7607s\n","399it [01:00,  6.70it/s]\titers: 400, epoch: 5 | loss: 0.0417941\n","\tspeed: 0.1513s/iter; left time: 7142.9386s\n","496it [01:14,  6.62it/s]\n","Epoch: 5 cost time: 75.2712414264679\n","49it [00:03, 15.02it/s]\n","49it [00:03, 14.84it/s]\n","Epoch: 5, Steps: 496 | Train Loss: 0.0456892 Vali Loss: 0.0400977 Test Loss: 0.0397375\n","Validation loss decreased (0.046749 --> 0.040098).  Saving model ...\n","Updating learning rate to 6.25e-05\n","99it [00:14,  6.67it/s]\titers: 100, epoch: 6 | loss: 0.0248237\n","\tspeed: 0.3778s/iter; left time: 17765.4211s\n","199it [00:29,  6.58it/s]\titers: 200, epoch: 6 | loss: 0.0291832\n","\tspeed: 0.1508s/iter; left time: 7076.4976s\n","299it [00:45,  6.65it/s]\titers: 300, epoch: 6 | loss: 0.0428496\n","\tspeed: 0.1513s/iter; left time: 7086.0429s\n","399it [01:00,  6.58it/s]\titers: 400, epoch: 6 | loss: 0.0410505\n","\tspeed: 0.1517s/iter; left time: 7085.3173s\n","496it [01:15,  6.60it/s]\n","Epoch: 6 cost time: 75.54054665565491\n","49it [00:03, 14.16it/s]\n","49it [00:03, 15.00it/s]\n","Epoch: 6, Steps: 496 | Train Loss: 0.0412830 Vali Loss: 0.0376953 Test Loss: 0.0379691\n","Validation loss decreased (0.040098 --> 0.037695).  Saving model ...\n","Updating learning rate to 3.125e-05\n","99it [00:14,  6.66it/s]\titers: 100, epoch: 7 | loss: 0.0174184\n","\tspeed: 0.3853s/iter; left time: 17928.3053s\n","199it [00:29,  6.60it/s]\titers: 200, epoch: 7 | loss: 0.0277469\n","\tspeed: 0.1505s/iter; left time: 6985.9689s\n","299it [00:44,  6.65it/s]\titers: 300, epoch: 7 | loss: 0.0421741\n","\tspeed: 0.1502s/iter; left time: 6958.5317s\n","399it [00:59,  6.67it/s]\titers: 400, epoch: 7 | loss: 0.0404602\n","\tspeed: 0.1505s/iter; left time: 6955.9518s\n","496it [01:14,  6.64it/s]\n","Epoch: 7 cost time: 75.05808472633362\n","49it [00:03, 14.75it/s]\n","49it [00:03, 14.82it/s]\n","Epoch: 7, Steps: 496 | Train Loss: 0.0390559 Vali Loss: 0.0368701 Test Loss: 0.0375027\n","Validation loss decreased (0.037695 --> 0.036870).  Saving model ...\n","Updating learning rate to 1.5625e-05\n","99it [00:14,  6.65it/s]\titers: 100, epoch: 8 | loss: 0.0192603\n","\tspeed: 0.3822s/iter; left time: 17591.2527s\n","199it [00:29,  6.65it/s]\titers: 200, epoch: 8 | loss: 0.0274786\n","\tspeed: 0.1502s/iter; left time: 6899.9385s\n","299it [00:44,  6.57it/s]\titers: 300, epoch: 8 | loss: 0.0385733\n","\tspeed: 0.1510s/iter; left time: 6918.7297s\n","399it [01:00,  6.58it/s]\titers: 400, epoch: 8 | loss: 0.0405570\n","\tspeed: 0.1512s/iter; left time: 6914.5843s\n","496it [01:15,  6.61it/s]\n","Epoch: 8 cost time: 75.59786581993103\n","49it [00:03, 13.91it/s]\n","49it [00:03, 15.04it/s]\n","Epoch: 8, Steps: 496 | Train Loss: 0.0381285 Vali Loss: 0.0365146 Test Loss: 0.0374486\n","Validation loss decreased (0.036870 --> 0.036515).  Saving model ...\n","Updating learning rate to 7.8125e-06\n","99it [00:14,  6.67it/s]\titers: 100, epoch: 9 | loss: 0.0178615\n","\tspeed: 0.3846s/iter; left time: 17512.1563s\n","199it [00:29,  6.52it/s]\titers: 200, epoch: 9 | loss: 0.0270405\n","\tspeed: 0.1501s/iter; left time: 6820.5533s\n","299it [00:44,  6.66it/s]\titers: 300, epoch: 9 | loss: 0.0358505\n","\tspeed: 0.1506s/iter; left time: 6825.9252s\n","399it [00:59,  6.63it/s]\titers: 400, epoch: 9 | loss: 0.0401993\n","\tspeed: 0.1503s/iter; left time: 6798.9935s\n","496it [01:14,  6.64it/s]\n","Epoch: 9 cost time: 75.03862738609314\n","49it [00:03, 14.88it/s]\n","49it [00:03, 14.29it/s]\n","Epoch: 9, Steps: 496 | Train Loss: 0.0375439 Vali Loss: 0.0362868 Test Loss: 0.0375106\n","Validation loss decreased (0.036515 --> 0.036287).  Saving model ...\n","Updating learning rate to 3.90625e-06\n","99it [00:14,  6.66it/s]\titers: 100, epoch: 10 | loss: 0.0171890\n","\tspeed: 0.3828s/iter; left time: 17241.2616s\n","199it [00:29,  6.53it/s]\titers: 200, epoch: 10 | loss: 0.0266091\n","\tspeed: 0.1513s/iter; left time: 6800.2198s\n","299it [00:45,  6.56it/s]\titers: 300, epoch: 10 | loss: 0.0366935\n","\tspeed: 0.1521s/iter; left time: 6820.7984s\n","399it [01:00,  6.65it/s]\titers: 400, epoch: 10 | loss: 0.0400987\n","\tspeed: 0.1517s/iter; left time: 6784.5037s\n","496it [01:15,  6.59it/s]\n","Epoch: 10 cost time: 75.7476167678833\n","49it [00:03, 15.00it/s]\n","49it [00:03, 15.01it/s]\n","Epoch: 10, Steps: 496 | Train Loss: 0.0372348 Vali Loss: 0.0362119 Test Loss: 0.0375439\n","Validation loss decreased (0.036287 --> 0.036212).  Saving model ...\n","Updating learning rate to 1.953125e-06\n","99it [00:14,  6.68it/s]\titers: 100, epoch: 11 | loss: 0.0171672\n","\tspeed: 0.3797s/iter; left time: 16910.5585s\n","199it [00:29,  6.65it/s]\titers: 200, epoch: 11 | loss: 0.0264420\n","\tspeed: 0.1500s/iter; left time: 6664.9760s\n","299it [00:44,  6.64it/s]\titers: 300, epoch: 11 | loss: 0.0379329\n","\tspeed: 0.1502s/iter; left time: 6660.0209s\n","399it [00:59,  6.64it/s]\titers: 400, epoch: 11 | loss: 0.0399251\n","\tspeed: 0.1504s/iter; left time: 6652.3632s\n","496it [01:14,  6.65it/s]\n","Epoch: 11 cost time: 74.95874309539795\n","49it [00:03, 14.17it/s]\n","49it [00:03, 13.78it/s]\n","Epoch: 11, Steps: 496 | Train Loss: 0.0370633 Vali Loss: 0.0363219 Test Loss: 0.0375928\n","EarlyStopping counter: 1 out of 3\n","Updating learning rate to 9.765625e-07\n","99it [00:14,  6.60it/s]\titers: 100, epoch: 12 | loss: 0.0176463\n","\tspeed: 0.3768s/iter; left time: 16594.7809s\n","199it [00:29,  6.67it/s]\titers: 200, epoch: 12 | loss: 0.0266987\n","\tspeed: 0.1495s/iter; left time: 6569.2129s\n","299it [00:44,  6.67it/s]\titers: 300, epoch: 12 | loss: 0.0364904\n","\tspeed: 0.1501s/iter; left time: 6579.0014s\n","399it [00:59,  6.62it/s]\titers: 400, epoch: 12 | loss: 0.0398764\n","\tspeed: 0.1513s/iter; left time: 6619.6104s\n","496it [01:14,  6.62it/s]\n","Epoch: 12 cost time: 75.23019504547119\n","49it [00:03, 14.90it/s]\n","49it [00:03, 14.84it/s]\n","Epoch: 12, Steps: 496 | Train Loss: 0.0369643 Vali Loss: 0.0364583 Test Loss: 0.0376321\n","EarlyStopping counter: 2 out of 3\n","Updating learning rate to 4.8828125e-07\n","99it [00:14,  6.58it/s]\titers: 100, epoch: 13 | loss: 0.0165091\n","\tspeed: 0.3745s/iter; left time: 16307.3270s\n","199it [00:30,  6.66it/s]\titers: 200, epoch: 13 | loss: 0.0264418\n","\tspeed: 0.1508s/iter; left time: 6553.6594s\n","299it [00:45,  6.64it/s]\titers: 300, epoch: 13 | loss: 0.0362941\n","\tspeed: 0.1503s/iter; left time: 6515.7478s\n","399it [01:00,  6.60it/s]\titers: 400, epoch: 13 | loss: 0.0396805\n","\tspeed: 0.1506s/iter; left time: 6513.3096s\n","496it [01:14,  6.62it/s]\n","Epoch: 13 cost time: 75.24826002120972\n","49it [00:03, 14.47it/s]\n","49it [00:03, 13.91it/s]\n","Epoch: 13, Steps: 496 | Train Loss: 0.0368467 Vali Loss: 0.0365324 Test Loss: 0.0376591\n","EarlyStopping counter: 3 out of 3\n","Early stopping\n",">>>>>>>testing : imputation_cha_all1_down1_mask_0.1_LLM4HRSI_custom_ftM_te100_bs16_gl1_dm768_nh8_el2_lr0.001_enci576_df768_fc1_ebtimeF_dtTrue_Exp_2<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n","0.1\n","test 798\n","49it [00:04, 11.13it/s]\n","test shape: (784, 24, 576) (784, 24, 576)\n","mse:0.004045581445097923, mae:0.03904634341597557,rmse:0.06360488384962082\n","Use GPU: cuda:0\n",">>>>>>>start training : imputation_cha_all1_down1_mask_0.1_LLM4HRSI_custom_ftM_te100_bs16_gl1_dm768_nh8_el2_lr0.001_enci576_df768_fc1_ebtimeF_dtTrue_Exp_3>>>>>>>>>>>>>>>>>>>>>>>>>>\n","0.1\n","train 7949\n","0.1\n","val 799\n","0.1\n","test 798\n","99it [00:15,  6.22it/s]\titers: 100, epoch: 1 | loss: 0.0760821\n","\tspeed: 0.1594s/iter; left time: 7891.0869s\n","199it [00:31,  6.39it/s]\titers: 200, epoch: 1 | loss: 0.0806948\n","\tspeed: 0.1604s/iter; left time: 7925.8151s\n","299it [00:46,  6.63it/s]\titers: 300, epoch: 1 | loss: 0.0928675\n","\tspeed: 0.1527s/iter; left time: 7530.1449s\n","399it [01:01,  6.69it/s]\titers: 400, epoch: 1 | loss: 0.0618898\n","\tspeed: 0.1483s/iter; left time: 7297.2589s\n","496it [01:16,  6.51it/s]\n","Epoch: 1 cost time: 76.48637962341309\n","49it [00:03, 14.91it/s]\n","49it [00:03, 15.03it/s]\n","Epoch: 1, Steps: 496 | Train Loss: 0.1007391 Vali Loss: 0.0558150 Test Loss: 0.0546925\n","Validation loss decreased (inf --> 0.055815).  Saving model ...\n","Updating learning rate to 0.001\n","99it [00:14,  6.55it/s]\titers: 100, epoch: 2 | loss: 0.0502191\n","\tspeed: 0.3800s/iter; left time: 18623.5523s\n","199it [00:30,  6.50it/s]\titers: 200, epoch: 2 | loss: 0.0550026\n","\tspeed: 0.1530s/iter; left time: 7483.9271s\n","299it [00:45,  6.55it/s]\titers: 300, epoch: 2 | loss: 0.0696378\n","\tspeed: 0.1529s/iter; left time: 7460.6532s\n","399it [01:00,  6.68it/s]\titers: 400, epoch: 2 | loss: 0.0525798\n","\tspeed: 0.1505s/iter; left time: 7331.6186s\n","496it [01:15,  6.59it/s]\n","Epoch: 2 cost time: 75.88665986061096\n","49it [00:03, 14.78it/s]\n","49it [00:03, 14.97it/s]\n","Epoch: 2, Steps: 496 | Train Loss: 0.0723455 Vali Loss: 0.0503068 Test Loss: 0.0507316\n","Validation loss decreased (0.055815 --> 0.050307).  Saving model ...\n","Updating learning rate to 0.0005\n","99it [00:14,  6.65it/s]\titers: 100, epoch: 3 | loss: 0.0301364\n","\tspeed: 0.3805s/iter; left time: 18456.3847s\n","199it [00:29,  6.59it/s]\titers: 200, epoch: 3 | loss: 0.0434562\n","\tspeed: 0.1514s/iter; left time: 7326.8867s\n","299it [00:45,  6.69it/s]\titers: 300, epoch: 3 | loss: 0.0770798\n","\tspeed: 0.1511s/iter; left time: 7299.8775s\n","399it [01:00,  6.63it/s]\titers: 400, epoch: 3 | loss: 0.0581333\n","\tspeed: 0.1502s/iter; left time: 7242.4938s\n","496it [01:14,  6.63it/s]\n","Epoch: 3 cost time: 75.18354368209839\n","49it [00:03, 14.80it/s]\n","49it [00:03, 15.01it/s]\n","Epoch: 3, Steps: 496 | Train Loss: 0.0624466 Vali Loss: 0.0555303 Test Loss: 0.0519896\n","EarlyStopping counter: 1 out of 3\n","Updating learning rate to 0.00025\n","99it [00:14,  6.61it/s]\titers: 100, epoch: 4 | loss: 0.0356331\n","\tspeed: 0.3718s/iter; left time: 17852.0846s\n","199it [00:30,  6.60it/s]\titers: 200, epoch: 4 | loss: 0.0334659\n","\tspeed: 0.1515s/iter; left time: 7260.7713s\n","299it [00:45,  6.62it/s]\titers: 300, epoch: 4 | loss: 0.0558799\n","\tspeed: 0.1513s/iter; left time: 7234.9371s\n","399it [01:00,  6.61it/s]\titers: 400, epoch: 4 | loss: 0.0441564\n","\tspeed: 0.1510s/iter; left time: 7206.4047s\n","496it [01:15,  6.60it/s]\n","Epoch: 4 cost time: 75.4589352607727\n","49it [00:03, 14.52it/s]\n","49it [00:03, 13.83it/s]\n","Epoch: 4, Steps: 496 | Train Loss: 0.0544598 Vali Loss: 0.0460943 Test Loss: 0.0439716\n","Validation loss decreased (0.050307 --> 0.046094).  Saving model ...\n","Updating learning rate to 0.000125\n","99it [00:14,  6.62it/s]\titers: 100, epoch: 5 | loss: 0.0262948\n","\tspeed: 0.3862s/iter; left time: 18349.7437s\n","199it [00:29,  6.62it/s]\titers: 200, epoch: 5 | loss: 0.0298133\n","\tspeed: 0.1509s/iter; left time: 7153.1683s\n","299it [00:45,  6.60it/s]\titers: 300, epoch: 5 | loss: 0.0438309\n","\tspeed: 0.1516s/iter; left time: 7171.5845s\n","399it [01:00,  6.56it/s]\titers: 400, epoch: 5 | loss: 0.0402995\n","\tspeed: 0.1517s/iter; left time: 7164.8727s\n","496it [01:15,  6.59it/s]\n","Epoch: 5 cost time: 75.54017496109009\n","49it [00:03, 15.01it/s]\n","49it [00:03, 15.07it/s]\n","Epoch: 5, Steps: 496 | Train Loss: 0.0429444 Vali Loss: 0.0388601 Test Loss: 0.0386739\n","Validation loss decreased (0.046094 --> 0.038860).  Saving model ...\n","Updating learning rate to 6.25e-05\n","99it [00:14,  6.63it/s]\titers: 100, epoch: 6 | loss: 0.0238520\n","\tspeed: 0.3800s/iter; left time: 17868.5966s\n","199it [00:29,  6.72it/s]\titers: 200, epoch: 6 | loss: 0.0286618\n","\tspeed: 0.1501s/iter; left time: 7041.2688s\n","299it [00:44,  6.57it/s]\titers: 300, epoch: 6 | loss: 0.0416570\n","\tspeed: 0.1511s/iter; left time: 7075.0107s\n","399it [01:00,  6.61it/s]\titers: 400, epoch: 6 | loss: 0.0396203\n","\tspeed: 0.1516s/iter; left time: 7083.5226s\n","496it [01:14,  6.62it/s]\n","Epoch: 6 cost time: 75.20254421234131\n","49it [00:03, 14.49it/s]\n","49it [00:03, 13.38it/s]\n","Epoch: 6, Steps: 496 | Train Loss: 0.0403675 Vali Loss: 0.0366112 Test Loss: 0.0374717\n","Validation loss decreased (0.038860 --> 0.036611).  Saving model ...\n","Updating learning rate to 3.125e-05\n","99it [00:14,  6.62it/s]\titers: 100, epoch: 7 | loss: 0.0189625\n","\tspeed: 0.3925s/iter; left time: 18263.3197s\n","199it [00:29,  6.60it/s]\titers: 200, epoch: 7 | loss: 0.0269973\n","\tspeed: 0.1502s/iter; left time: 6972.6143s\n","299it [00:44,  6.55it/s]\titers: 300, epoch: 7 | loss: 0.0359478\n","\tspeed: 0.1512s/iter; left time: 7005.0806s\n","399it [01:00,  6.61it/s]\titers: 400, epoch: 7 | loss: 0.0400985\n","\tspeed: 0.1517s/iter; left time: 7013.8169s\n","496it [01:15,  6.61it/s]\n","Epoch: 7 cost time: 75.59356927871704\n","49it [00:03, 14.93it/s]\n","49it [00:03, 14.92it/s]\n","Epoch: 7, Steps: 496 | Train Loss: 0.0381116 Vali Loss: 0.0357944 Test Loss: 0.0369782\n","Validation loss decreased (0.036611 --> 0.035794).  Saving model ...\n","Updating learning rate to 1.5625e-05\n","99it [00:14,  6.66it/s]\titers: 100, epoch: 8 | loss: 0.0180539\n","\tspeed: 0.3803s/iter; left time: 17503.4266s\n","199it [00:29,  6.70it/s]\titers: 200, epoch: 8 | loss: 0.0264638\n","\tspeed: 0.1496s/iter; left time: 6869.2959s\n","299it [00:44,  6.58it/s]\titers: 300, epoch: 8 | loss: 0.0368224\n","\tspeed: 0.1503s/iter; left time: 6888.5493s\n","399it [00:59,  6.59it/s]\titers: 400, epoch: 8 | loss: 0.0391298\n","\tspeed: 0.1508s/iter; left time: 6897.2270s\n","496it [01:14,  6.64it/s]\n","Epoch: 8 cost time: 75.09880089759827\n","49it [00:03, 13.98it/s]\n","49it [00:03, 14.95it/s]\n","Epoch: 8, Steps: 496 | Train Loss: 0.0368979 Vali Loss: 0.0353961 Test Loss: 0.0368688\n","Validation loss decreased (0.035794 --> 0.035396).  Saving model ...\n","Updating learning rate to 7.8125e-06\n","99it [00:14,  6.63it/s]\titers: 100, epoch: 9 | loss: 0.0169825\n","\tspeed: 0.3846s/iter; left time: 17513.7460s\n","199it [00:29,  6.63it/s]\titers: 200, epoch: 9 | loss: 0.0255655\n","\tspeed: 0.1505s/iter; left time: 6835.9348s\n","299it [00:44,  6.61it/s]\titers: 300, epoch: 9 | loss: 0.0372497\n","\tspeed: 0.1512s/iter; left time: 6854.4083s\n","399it [01:00,  6.61it/s]\titers: 400, epoch: 9 | loss: 0.0390226\n","\tspeed: 0.1517s/iter; left time: 6859.7367s\n","496it [01:15,  6.61it/s]\n","Epoch: 9 cost time: 75.39769697189331\n","49it [00:03, 14.80it/s]\n","49it [00:03, 14.26it/s]\n","Epoch: 9, Steps: 496 | Train Loss: 0.0364667 Vali Loss: 0.0352441 Test Loss: 0.0369160\n","Validation loss decreased (0.035396 --> 0.035244).  Saving model ...\n","Updating learning rate to 3.90625e-06\n","99it [00:14,  6.66it/s]\titers: 100, epoch: 10 | loss: 0.0161481\n","\tspeed: 0.3857s/iter; left time: 17372.0174s\n","199it [00:29,  6.57it/s]\titers: 200, epoch: 10 | loss: 0.0254344\n","\tspeed: 0.1511s/iter; left time: 6790.1538s\n","299it [00:45,  6.64it/s]\titers: 300, epoch: 10 | loss: 0.0336105\n","\tspeed: 0.1515s/iter; left time: 6791.8314s\n","399it [01:00,  6.63it/s]\titers: 400, epoch: 10 | loss: 0.0389377\n","\tspeed: 0.1510s/iter; left time: 6753.4149s\n","496it [01:15,  6.61it/s]\n","Epoch: 10 cost time: 75.49686646461487\n","49it [00:03, 15.10it/s]\n","49it [00:03, 14.93it/s]\n","Epoch: 10, Steps: 496 | Train Loss: 0.0361490 Vali Loss: 0.0352655 Test Loss: 0.0370080\n","EarlyStopping counter: 1 out of 3\n","Updating learning rate to 1.953125e-06\n","99it [00:14,  6.65it/s]\titers: 100, epoch: 11 | loss: 0.0154020\n","\tspeed: 0.3717s/iter; left time: 16557.7767s\n","199it [00:29,  6.57it/s]\titers: 200, epoch: 11 | loss: 0.0260195\n","\tspeed: 0.1506s/iter; left time: 6694.0499s\n","299it [00:45,  6.58it/s]\titers: 300, epoch: 11 | loss: 0.0349887\n","\tspeed: 0.1517s/iter; left time: 6726.8380s\n","399it [01:00,  6.59it/s]\titers: 400, epoch: 11 | loss: 0.0387021\n","\tspeed: 0.1512s/iter; left time: 6690.5511s\n","496it [01:14,  6.62it/s]\n","Epoch: 11 cost time: 75.23958778381348\n","49it [00:03, 14.40it/s]\n","49it [00:03, 13.90it/s]\n","Epoch: 11, Steps: 496 | Train Loss: 0.0360600 Vali Loss: 0.0354629 Test Loss: 0.0370927\n","EarlyStopping counter: 2 out of 3\n","Updating learning rate to 9.765625e-07\n","99it [00:14,  6.61it/s]\titers: 100, epoch: 12 | loss: 0.0181463\n","\tspeed: 0.3780s/iter; left time: 16649.4163s\n","199it [00:30,  6.58it/s]\titers: 200, epoch: 12 | loss: 0.0251172\n","\tspeed: 0.1513s/iter; left time: 6648.8292s\n","299it [00:45,  6.58it/s]\titers: 300, epoch: 12 | loss: 0.0328443\n","\tspeed: 0.1512s/iter; left time: 6631.4499s\n","399it [01:00,  6.60it/s]\titers: 400, epoch: 12 | loss: 0.0385801\n","\tspeed: 0.1508s/iter; left time: 6596.8332s\n","496it [01:14,  6.61it/s]\n","Epoch: 12 cost time: 75.41927933692932\n","49it [00:03, 15.00it/s]\n","49it [00:03, 14.87it/s]\n","Epoch: 12, Steps: 496 | Train Loss: 0.0358555 Vali Loss: 0.0356364 Test Loss: 0.0371571\n","EarlyStopping counter: 3 out of 3\n","Early stopping\n",">>>>>>>testing : imputation_cha_all1_down1_mask_0.1_LLM4HRSI_custom_ftM_te100_bs16_gl1_dm768_nh8_el2_lr0.001_enci576_df768_fc1_ebtimeF_dtTrue_Exp_3<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n","0.1\n","test 798\n","49it [00:04, 11.79it/s]\n","test shape: (784, 24, 576) (784, 24, 576)\n","mse:0.0039741951040923595, mae:0.038574207574129105,rmse:0.06304121762514114\n","Use GPU: cuda:0\n",">>>>>>>start training : imputation_cha_all1_down1_mask_0.1_LLM4HRSI_custom_ftM_te100_bs16_gl1_dm768_nh8_el2_lr0.001_enci576_df768_fc1_ebtimeF_dtTrue_Exp_4>>>>>>>>>>>>>>>>>>>>>>>>>>\n","0.1\n","train 7949\n","0.1\n","val 799\n","0.1\n","test 798\n","99it [00:14,  6.49it/s]\titers: 100, epoch: 1 | loss: 0.0865843\n","\tspeed: 0.1542s/iter; left time: 7632.3402s\n","199it [00:30,  6.15it/s]\titers: 200, epoch: 1 | loss: 0.0993318\n","\tspeed: 0.1582s/iter; left time: 7813.4833s\n","299it [00:46,  6.54it/s]\titers: 300, epoch: 1 | loss: 0.0877925\n","\tspeed: 0.1584s/iter; left time: 7808.0201s\n","399it [01:01,  6.75it/s]\titers: 400, epoch: 1 | loss: 0.0613760\n","\tspeed: 0.1507s/iter; left time: 7415.8882s\n","496it [01:16,  6.52it/s]\n","Epoch: 1 cost time: 76.56011629104614\n","49it [00:03, 13.88it/s]\n","49it [00:03, 15.04it/s]\n","Epoch: 1, Steps: 496 | Train Loss: 0.1094378 Vali Loss: 0.0551035 Test Loss: 0.0520906\n","Validation loss decreased (inf --> 0.055103).  Saving model ...\n","Updating learning rate to 0.001\n","99it [00:14,  6.57it/s]\titers: 100, epoch: 2 | loss: 0.0678505\n","\tspeed: 0.3808s/iter; left time: 18661.5257s\n","199it [00:30,  6.53it/s]\titers: 200, epoch: 2 | loss: 0.0633908\n","\tspeed: 0.1526s/iter; left time: 7465.0584s\n","299it [00:45,  6.54it/s]\titers: 300, epoch: 2 | loss: 0.0815786\n","\tspeed: 0.1532s/iter; left time: 7477.5244s\n","399it [01:00,  6.64it/s]\titers: 400, epoch: 2 | loss: 0.0533308\n","\tspeed: 0.1519s/iter; left time: 7396.0994s\n","496it [01:15,  6.59it/s]\n","Epoch: 2 cost time: 75.68023037910461\n","49it [00:03, 14.76it/s]\n","49it [00:03, 14.25it/s]\n","Epoch: 2, Steps: 496 | Train Loss: 0.0846647 Vali Loss: 0.0549326 Test Loss: 0.0527655\n","Validation loss decreased (0.055103 --> 0.054933).  Saving model ...\n","Updating learning rate to 0.0005\n","99it [00:14,  6.67it/s]\titers: 100, epoch: 3 | loss: 0.0432595\n","\tspeed: 0.3867s/iter; left time: 18759.4270s\n","199it [00:29,  6.58it/s]\titers: 200, epoch: 3 | loss: 0.0427602\n","\tspeed: 0.1512s/iter; left time: 7317.7268s\n","299it [00:45,  6.61it/s]\titers: 300, epoch: 3 | loss: 0.0657598\n","\tspeed: 0.1520s/iter; left time: 7343.3170s\n","399it [01:00,  6.60it/s]\titers: 400, epoch: 3 | loss: 0.0601915\n","\tspeed: 0.1513s/iter; left time: 7295.7000s\n","496it [01:15,  6.60it/s]\n","Epoch: 3 cost time: 75.81724500656128\n","49it [00:03, 13.82it/s]\n","49it [00:03, 14.90it/s]\n","Epoch: 3, Steps: 496 | Train Loss: 0.0760273 Vali Loss: 0.0570874 Test Loss: 0.0537726\n","EarlyStopping counter: 1 out of 3\n","Updating learning rate to 0.00025\n","99it [00:14,  6.70it/s]\titers: 100, epoch: 4 | loss: 0.0378019\n","\tspeed: 0.3783s/iter; left time: 18161.1414s\n","199it [00:29,  6.62it/s]\titers: 200, epoch: 4 | loss: 0.0316728\n","\tspeed: 0.1507s/iter; left time: 7221.5862s\n","299it [00:44,  6.62it/s]\titers: 300, epoch: 4 | loss: 0.0522751\n","\tspeed: 0.1516s/iter; left time: 7249.6399s\n","399it [01:00,  6.58it/s]\titers: 400, epoch: 4 | loss: 0.0448075\n","\tspeed: 0.1518s/iter; left time: 7241.6093s\n","496it [01:15,  6.60it/s]\n","Epoch: 4 cost time: 75.49724984169006\n","49it [00:03, 14.85it/s]\n","49it [00:03, 14.85it/s]\n","Epoch: 4, Steps: 496 | Train Loss: 0.0556989 Vali Loss: 0.0449895 Test Loss: 0.0431218\n","Validation loss decreased (0.054933 --> 0.044989).  Saving model ...\n","Updating learning rate to 0.000125\n","99it [00:15,  6.69it/s]\titers: 100, epoch: 5 | loss: 0.0269508\n","\tspeed: 0.3846s/iter; left time: 18275.8665s\n","199it [00:30,  6.62it/s]\titers: 200, epoch: 5 | loss: 0.0299735\n","\tspeed: 0.1498s/iter; left time: 7103.0550s\n","299it [00:45,  6.60it/s]\titers: 300, epoch: 5 | loss: 0.0487231\n","\tspeed: 0.1501s/iter; left time: 7103.1625s\n","399it [01:00,  6.65it/s]\titers: 400, epoch: 5 | loss: 0.0420130\n","\tspeed: 0.1504s/iter; left time: 7099.3844s\n","496it [01:15,  6.61it/s]\n","Epoch: 5 cost time: 75.31766510009766\n","49it [00:03, 14.18it/s]\n","49it [00:03, 13.65it/s]\n","Epoch: 5, Steps: 496 | Train Loss: 0.0484088 Vali Loss: 0.0400145 Test Loss: 0.0393937\n","Validation loss decreased (0.044989 --> 0.040015).  Saving model ...\n","Updating learning rate to 6.25e-05\n","99it [00:14,  6.66it/s]\titers: 100, epoch: 6 | loss: 0.0239918\n","\tspeed: 0.3869s/iter; left time: 18191.3526s\n","199it [00:30,  6.56it/s]\titers: 200, epoch: 6 | loss: 0.0292248\n","\tspeed: 0.1513s/iter; left time: 7100.5295s\n","299it [00:45,  6.54it/s]\titers: 300, epoch: 6 | loss: 0.0448085\n","\tspeed: 0.1518s/iter; left time: 7107.8181s\n","399it [01:00,  6.58it/s]\titers: 400, epoch: 6 | loss: 0.0408629\n","\tspeed: 0.1517s/iter; left time: 7085.3765s\n","496it [01:15,  6.59it/s]\n","Epoch: 6 cost time: 75.67168760299683\n","49it [00:03, 14.96it/s]\n","49it [00:03, 14.96it/s]\n","Epoch: 6, Steps: 496 | Train Loss: 0.0414126 Vali Loss: 0.0380788 Test Loss: 0.0382604\n","Validation loss decreased (0.040015 --> 0.038079).  Saving model ...\n","Updating learning rate to 3.125e-05\n","99it [00:14,  6.69it/s]\titers: 100, epoch: 7 | loss: 0.0202446\n","\tspeed: 0.3797s/iter; left time: 17667.7652s\n","199it [00:29,  6.63it/s]\titers: 200, epoch: 7 | loss: 0.0278154\n","\tspeed: 0.1499s/iter; left time: 6957.2967s\n","299it [00:44,  6.69it/s]\titers: 300, epoch: 7 | loss: 0.0416519\n","\tspeed: 0.1502s/iter; left time: 6959.9831s\n","399it [00:59,  6.60it/s]\titers: 400, epoch: 7 | loss: 0.0406085\n","\tspeed: 0.1509s/iter; left time: 6974.1052s\n","496it [01:14,  6.64it/s]\n","Epoch: 7 cost time: 75.11737275123596\n","49it [00:03, 14.03it/s]\n","49it [00:03, 13.64it/s]\n","Epoch: 7, Steps: 496 | Train Loss: 0.0398533 Vali Loss: 0.0373147 Test Loss: 0.0379171\n","Validation loss decreased (0.038079 --> 0.037315).  Saving model ...\n","Updating learning rate to 1.5625e-05\n","99it [00:14,  6.66it/s]\titers: 100, epoch: 8 | loss: 0.0198323\n","\tspeed: 0.3878s/iter; left time: 17850.6556s\n","199it [00:30,  6.51it/s]\titers: 200, epoch: 8 | loss: 0.0277847\n","\tspeed: 0.1514s/iter; left time: 6954.3065s\n","299it [00:45,  6.57it/s]\titers: 300, epoch: 8 | loss: 0.0400054\n","\tspeed: 0.1516s/iter; left time: 6949.5798s\n","399it [01:00,  6.58it/s]\titers: 400, epoch: 8 | loss: 0.0407463\n","\tspeed: 0.1519s/iter; left time: 6945.8617s\n","496it [01:15,  6.59it/s]\n","Epoch: 8 cost time: 75.68120908737183\n","49it [00:03, 14.69it/s]\n","49it [00:03, 14.26it/s]\n","Epoch: 8, Steps: 496 | Train Loss: 0.0388402 Vali Loss: 0.0371217 Test Loss: 0.0379108\n","Validation loss decreased (0.037315 --> 0.037122).  Saving model ...\n","Updating learning rate to 7.8125e-06\n","99it [00:14,  6.70it/s]\titers: 100, epoch: 9 | loss: 0.0182574\n","\tspeed: 0.3866s/iter; left time: 17604.6385s\n","199it [00:29,  6.61it/s]\titers: 200, epoch: 9 | loss: 0.0272508\n","\tspeed: 0.1502s/iter; left time: 6824.5507s\n","299it [00:44,  6.66it/s]\titers: 300, epoch: 9 | loss: 0.0396809\n","\tspeed: 0.1499s/iter; left time: 6793.3918s\n","399it [00:59,  6.68it/s]\titers: 400, epoch: 9 | loss: 0.0403321\n","\tspeed: 0.1503s/iter; left time: 6797.4288s\n","496it [01:14,  6.63it/s]\n","Epoch: 9 cost time: 75.42866086959839\n","49it [00:03, 13.74it/s]\n","49it [00:03, 15.09it/s]\n","Epoch: 9, Steps: 496 | Train Loss: 0.0383901 Vali Loss: 0.0370650 Test Loss: 0.0379571\n","Validation loss decreased (0.037122 --> 0.037065).  Saving model ...\n","Updating learning rate to 3.90625e-06\n","99it [00:14,  6.66it/s]\titers: 100, epoch: 10 | loss: 0.0181097\n","\tspeed: 0.3862s/iter; left time: 17394.2329s\n","199it [00:29,  6.62it/s]\titers: 200, epoch: 10 | loss: 0.0273466\n","\tspeed: 0.1508s/iter; left time: 6776.6147s\n","299it [00:45,  6.60it/s]\titers: 300, epoch: 10 | loss: 0.0393430\n","\tspeed: 0.1514s/iter; left time: 6790.4018s\n","399it [01:00,  6.56it/s]\titers: 400, epoch: 10 | loss: 0.0407753\n","\tspeed: 0.1519s/iter; left time: 6794.6258s\n","496it [01:15,  6.60it/s]\n","Epoch: 10 cost time: 75.5379467010498\n","49it [00:03, 14.84it/s]\n","49it [00:03, 14.32it/s]\n","Epoch: 10, Steps: 496 | Train Loss: 0.0381511 Vali Loss: 0.0371616 Test Loss: 0.0380899\n","EarlyStopping counter: 1 out of 3\n","Updating learning rate to 1.953125e-06\n","99it [00:14,  6.63it/s]\titers: 100, epoch: 11 | loss: 0.0186503\n","\tspeed: 0.3767s/iter; left time: 16780.2194s\n","199it [00:29,  6.61it/s]\titers: 200, epoch: 11 | loss: 0.0271787\n","\tspeed: 0.1501s/iter; left time: 6671.9191s\n","299it [00:44,  6.62it/s]\titers: 300, epoch: 11 | loss: 0.0392943\n","\tspeed: 0.1503s/iter; left time: 6664.6437s\n","399it [00:59,  6.60it/s]\titers: 400, epoch: 11 | loss: 0.0406783\n","\tspeed: 0.1506s/iter; left time: 6663.5824s\n","496it [01:14,  6.62it/s]\n","Epoch: 11 cost time: 75.4048478603363\n","49it [00:03, 13.67it/s]\n","49it [00:03, 14.79it/s]\n","Epoch: 11, Steps: 496 | Train Loss: 0.0380965 Vali Loss: 0.0374133 Test Loss: 0.0382296\n","EarlyStopping counter: 2 out of 3\n","Updating learning rate to 9.765625e-07\n","99it [00:14,  6.77it/s]\titers: 100, epoch: 12 | loss: 0.0173551\n","\tspeed: 0.3800s/iter; left time: 16735.4945s\n","199it [00:29,  6.60it/s]\titers: 200, epoch: 12 | loss: 0.0270233\n","\tspeed: 0.1506s/iter; left time: 6616.9629s\n","299it [00:44,  6.65it/s]\titers: 300, epoch: 12 | loss: 0.0375352\n","\tspeed: 0.1510s/iter; left time: 6619.1009s\n","399it [01:00,  6.61it/s]\titers: 400, epoch: 12 | loss: 0.0405886\n","\tspeed: 0.1513s/iter; left time: 6617.5725s\n","496it [01:15,  6.60it/s]\n","Epoch: 12 cost time: 75.48950028419495\n","49it [00:03, 13.61it/s]\n","49it [00:03, 14.27it/s]\n","Epoch: 12, Steps: 496 | Train Loss: 0.0379033 Vali Loss: 0.0376147 Test Loss: 0.0383356\n","EarlyStopping counter: 3 out of 3\n","Early stopping\n",">>>>>>>testing : imputation_cha_all1_down1_mask_0.1_LLM4HRSI_custom_ftM_te100_bs16_gl1_dm768_nh8_el2_lr0.001_enci576_df768_fc1_ebtimeF_dtTrue_Exp_4<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n","0.1\n","test 798\n","49it [00:04, 11.93it/s]\n","test shape: (784, 24, 576) (784, 24, 576)\n","mse:0.004118056036531925, mae:0.039497703313827515,rmse:0.06417208164930344\n"]}],"source":["!bash ./scripts/CHL-A1.sh"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyM7QbWMszIGu14aIiEE43m5","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
